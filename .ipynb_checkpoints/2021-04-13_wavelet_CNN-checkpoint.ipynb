{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ready-norway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal\n",
    "import pywt\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import h5py\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import tensorflow_io as tfio\n",
    "from tensorflow.keras.callbacks import History\n",
    "\n",
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "neither-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(accDir, annotFile):\n",
    "    files = os.listdir(accDir)\n",
    "    files_csv = [f for f in files if f[-3:] == 'csv']\n",
    "    empatica_dict = dict()\n",
    "    for f in files_csv:\n",
    "        data = np.genfromtxt(accDir+f, delimiter=',') # creates numpy array for each Empatica acc csv file\n",
    "        key = int(float(f.strip(\"ACC.csv\")))\n",
    "        empatica_dict[key] = data\n",
    "    tmp = pd.read_excel(annotFile, sheet_name=None)\n",
    "    annot_dict = dict(zip(tmp.keys(), [i.dropna() for i in tmp.values()])) # Remove the rows with NaN values (some with ladder 2 missing)\n",
    "    return empatica_dict, annot_dict\n",
    "\n",
    "def getLabeledDict(empatica_dict, annot_dict, subject_ids):\n",
    "    labeled_dict = {}; taskInd_dict = {}\n",
    "    for id in subject_ids:\n",
    "        start_time = int(empatica_dict[id][0,0])\n",
    "        acc = empatica_dict[id][2:,:]\n",
    "        label = list(map(lambda i: i.replace(\"_end\", \"\").replace(\"_start\", \"\"), annot_dict['P'+ str(id)].taskName.tolist()))\n",
    "        task_time= list(map(lambda i: time.mktime(datetime.datetime.strptime(i[:6] + '20' + i[6:], \"%m/%d/%Y %H:%M:%S\").timetuple()),\n",
    "                            annot_dict['P'+ str(id)].startTime_global.tolist()))\n",
    "        task_ind = [int(x - start_time)*SR for x in task_time]\n",
    "        taskInd_dict[id] = task_ind\n",
    "        label_tmp = np.empty(acc.shape[0], dtype=object)\n",
    "        for i, (j, k) in enumerate(zip(task_ind[0::2], task_ind[1::2])):\n",
    "            tmpInd = 2*i\n",
    "            label_tmp[j:k] = label[tmpInd]\n",
    "        acc_mag = np.sqrt(np.sum(acc**2, axis=1))[:,None]\n",
    "        accel = np.hstack((acc, acc_mag))\n",
    "        labeled_dict[id] = pd.DataFrame(np.hstack((accel, label_tmp.reshape(label_tmp.shape[0],1))), columns=['X', 'Y', 'Z', 'Mag', 'label'])\n",
    "    return labeled_dict, taskInd_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "delayed-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "sepAccDict, sepAnnotDict = readData(accDir='./Data/50_subs/Acc Data/separate/', annotFile='./Data/50_subs/Annotation Data/separate.xlsx')\n",
    "SR=int(sepAccDict[8][1,0])\n",
    "\n",
    "sepSubIDs = list(range(8,45))\n",
    "# sepSubIDs.remove(27) # does not have lift\n",
    "sepLabeledDict_, sepTaskIndDict = getLabeledDict(sepAccDict, sepAnnotDict, sepSubIDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-valve",
   "metadata": {},
   "source": [
    "## Apply Low Pass Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "broadband-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Filter on All Subjects\n",
    "n=4; fc=2; w=fc/(SR/2)\n",
    "b, a = signal.butter(n, w, 'low')\n",
    "sepLabeledDict_filtered = dict(map(lambda key: (key, signal.filtfilt(b, a, x=sepLabeledDict_[key].drop(columns='label'), axis=0)), sepLabeledDict_.keys()))\n",
    "# back to DF and add label\n",
    "sepLabeledDict_filtered_dfs = dict(map(lambda key: (\n",
    "                                                        key, pd.DataFrame(sepLabeledDict_filtered[key],columns=['X', 'Y', 'Z', 'Mag']).assign(label=sepLabeledDict_[key].label)\n",
    "                                                    ), sepLabeledDict_filtered.keys()))\n",
    "# Remove data without label\n",
    "filt_noNA_dict = dict(map(lambda key: (key, sepLabeledDict_filtered_dfs[key].dropna()), sepLabeledDict_filtered_dfs.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "immune-correction",
   "metadata": {},
   "outputs": [],
   "source": [
    "winLen = 320\n",
    "window_dict = {}\n",
    "label_dict = {}\n",
    "for key in filt_noNA_dict.keys():\n",
    "    window_list = []\n",
    "    labels=[]\n",
    "    for g1, df1 in filt_noNA_dict[key].groupby('label'):\n",
    "        for g2, df2 in df1.groupby(np.arange(df1.shape[0]) // winLen):\n",
    "            if df2.shape[0]==winLen:\n",
    "                window_list.append(df2.drop(columns=['Mag', 'label']))\n",
    "                labels.append(g1)\n",
    "    window_dict[key] = np.array(window_list)\n",
    "    label_dict[key] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "million-matter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, (187, 320, 3))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(window_dict[8]), window_dict[8].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-shirt",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "legal-corruption",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2021)\n",
    "percentTrain = 80\n",
    "all_subs = list(label_dict.keys())\n",
    "train_subs = random.sample(all_subs, k=int(len(all_subs)*(percentTrain/100)))\n",
    "test_subs = list(set(all_subs) - set(train_subs))\n",
    "\n",
    "train_array_list = [window_dict[key] for key in train_subs]\n",
    "test_array_list = [window_dict[key] for key in test_subs]\n",
    "train_np = np.concatenate(train_array_list)\n",
    "test_np = np.concatenate(test_array_list)\n",
    "\n",
    "train_label__ = [label_dict[key] for key in train_subs]\n",
    "train_label_ = [item for sublist in train_label__ for item in sublist]\n",
    "train_label = [item.replace('1', '').replace('2', '') for item in train_label_]\n",
    "test_label__ = [label_dict[key] for key in test_subs]\n",
    "test_label_ = [item for sublist in test_label__ for item in sublist]\n",
    "test_label = [item.replace('1', '').replace('2', '') for item in test_label_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "usual-season",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29, 8, 37)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_subs), len(test_subs), len(all_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "consistent-timer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9, 10, 11, 12, 13, 15, 16, 17, 18, 22, 23, 24, 25, 26, 27, 28, 30, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]\n",
      "[8, 14, 19, 20, 21, 29, 31, 32]\n",
      "[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(train_subs))\n",
    "print(sorted(test_subs))\n",
    "print(sorted(all_subs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medieval-exploration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "consistent-tours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(2021)\n",
    "# percent_split = 80\n",
    "# all_subs = list(label_dict.keys())\n",
    "# non_test_subs = random.sample(all_subs, k=int(len(all_subs)*(percent_split/100)))\n",
    "# test_subs = list(set(all_subs) - set(non_test_subs))\n",
    "# train_subs = random.sample(non_test_subs, k=int(len(non_test_subs)*(percent_split/100)))\n",
    "# val_subs = list(set(non_test_subs) - set(train_subs))\n",
    "\n",
    "# with open('test_subs.pickle', 'wb') as outfile:\n",
    "#     pickle.dump(test_subs, outfile)\n",
    "\n",
    "# train_array_list = [window_dict[key] for key in train_subs]\n",
    "# val_array_list = [window_dict[key] for key in val_subs]\n",
    "# train_np = np.concatenate(train_array_list)\n",
    "# val_np = np.concatenate(val_array_list)\n",
    "\n",
    "# train_label__ = [label_dict[key] for key in train_subs]\n",
    "# train_label_ = [item for sublist in train_label__ for item in sublist]\n",
    "# train_label = [item.replace('1', '').replace('2', '') for item in train_label_]\n",
    "# val_label__ = [label_dict[key] for key in val_subs]\n",
    "# val_label_ = [item for sublist in val_label__ for item in sublist]\n",
    "# val_label = [item.replace('1', '').replace('2', '') for item in val_label_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "finite-chester",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 28, list, 232)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_label__), len(train_label__), type(train_label__[0]), len(train_label__[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "designing-network",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 6070)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_label_), len(train_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "hollow-audio",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'electricPanel',\n",
       " 'hoist',\n",
       " 'ladder1',\n",
       " 'ladder2',\n",
       " 'lift',\n",
       " 'overhead',\n",
       " 'push',\n",
       " 'sit',\n",
       " 'stand',\n",
       " 'type',\n",
       " 'walk'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "comprehensive-mercy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'electricPanel',\n",
       " 'hoist',\n",
       " 'ladder',\n",
       " 'lift',\n",
       " 'overhead',\n",
       " 'push',\n",
       " 'sit',\n",
       " 'stand',\n",
       " 'type',\n",
       " 'walk'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "smooth-firmware",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6070, 320, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_np.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-amino",
   "metadata": {},
   "source": [
    "# Extract Continuous Wavelet Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "upset-ethernet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "0\n",
      "1000\n",
      "elapsed time = 1542.5610084533691\n"
     ]
    }
   ],
   "source": [
    "# start_time = time.time()\n",
    "\n",
    "# SR = 1/32\n",
    "# scales = range(1,winLen)\n",
    "# waveletname = 'morl'\n",
    "\n",
    "# with h5py.File('data1.hdf5', 'w') as hf:\n",
    "#     hf.create_dataset('data_train', (train_np.shape[0], winLen-1, winLen-1, train_np.shape[2]), np.float)\n",
    "#     for i in range(train_np.shape[0]):\n",
    "#         if i % 1000 == 0:\n",
    "#             print(i)\n",
    "#         for j in range(train_np.shape[2]):\n",
    "#             sig = train_np[i,:,j]\n",
    "#             coeff, freq = pywt.cwt(sig, scales, waveletname, SR)\n",
    "#             hf[\"data_train\"][i, :, :, j] = coeff[:, :-1]\n",
    "            \n",
    "#     hf.create_dataset('data_test', (test_np.shape[0], winLen-1, winLen-1, test_np.shape[2]), np.float)\n",
    "#     for i in range(test_np.shape[0]):\n",
    "#         if i % 1000 == 0:\n",
    "#             print(i)\n",
    "#         for j in range(test_np.shape[2]):\n",
    "#             sig = test_np[i,:,j]\n",
    "#             coeff, freq = pywt.cwt(sig, scales, waveletname, SR)\n",
    "#             hf[\"data_test\"][i, :, :, j] = coeff[:, :-1]\n",
    "            \n",
    "# print('elapsed time = {}'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "centered-counter",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_integer_encoded = LabelEncoder().fit_transform(train_label)\n",
    "y_test_integer_encoded = LabelEncoder().fit_transform(test_label)\n",
    "y_train = keras.utils.to_categorical(y_train_integer_encoded, 10)\n",
    "y_test = keras.utils.to_categorical(y_test_integer_encoded, 10)\n",
    "# with h5py.File('data_label.hdf5', 'w') as hf:\n",
    "#     hf['data_train'] = y_train\n",
    "#     hf['data_test'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "clean-anime",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lift', 'stand', 'type', 'walk', 'overhead', 'push', 'electricPanel', 'ladder', 'hoist', 'sit'}\n",
      "{'lift', 'stand', 'type', 'walk', 'overhead', 'push', 'electricPanel', 'ladder', 'hoist', 'sit'}\n"
     ]
    }
   ],
   "source": [
    "print(set(train_label))\n",
    "print(set(test_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "controlling-first",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 2, 3, 4], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = ['1', '2', '3','3', '4', '5']\n",
    "\n",
    "aa_encoded = LabelEncoder().fit_transform(aa)\n",
    "aa_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "gothic-degree",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 4, 5, 1, 7, 6, 0, 8, 2, 3, 1], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bb = ['3', '4', '5', '1', 'blah', 'a', '0', 'blah2', 23, 3, 1]\n",
    "bb_encoded = LabelEncoder().fit_transform(bb)\n",
    "bb_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "creative-probability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.to_categorical(bb_encoded, len(bb_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "graduate-journalism",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.unique(bb_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "interim-underground",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6070, 10), 6070)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "material-permission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 9, 9, 9], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train_integer_encoded), y_train_integer_encoded.shape\n",
    "y_train_integer_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "patent-jerusalem",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6065</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6066</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6067</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6068</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6069</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6070 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0    1    2    3    4    5    6    7    8    9\n",
       "0     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "2     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "3     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "4     1.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "...   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...\n",
       "6065  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
       "6066  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
       "6067  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
       "6068  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
       "6069  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0\n",
       "\n",
       "[6070 rows x 10 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd. DataFrame(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "stylish-chile",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h5py.File('data1.hdf5', 'r')\n",
    "aa_ = h.get('data_train')\n",
    "aa = aa_[:,:,:,:]\n",
    "# train_h5 = h['data_train']\n",
    "test_h5 = h.get('data_test')\n",
    "# test_h5 = h['data_test']\n",
    "h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "august-break",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Not a dataset (not a dataset)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-c6f667e3e5c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maa_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maa_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\h5py\\_hl\\dataset.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[1;34m\"\"\"Numpy-style shape tuple giving dataset dimensions\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mwith_phil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.shape.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.shape.__get__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5d.pyx\u001b[0m in \u001b[0;36mh5py.h5d.DatasetID.get_space\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Not a dataset (not a dataset)"
     ]
    }
   ],
   "source": [
    "type(aa_), aa_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "retained-emphasis",
   "metadata": {},
   "outputs": [],
   "source": [
    "del aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "wrapped-constitution",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.08871858628964163"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa[0,0,10,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "matched-projection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6070, 319, 319, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_h5.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "incorrect-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = np.zeros(shape=(train_np.shape[0], winLen-1, winLen-1, train_np.shape[2]), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "threatened-garbage",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float64')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-stylus",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# f, ax = plt.subplots(1,figsize=(7,7))\n",
    "# ax.imshow(train_h5[0,:,:,:])#, cmap=plt.cm.seismic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-suffering",
   "metadata": {},
   "source": [
    "# CNN Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "honest-thompson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "pharmaceutical-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h5py.File('data1.hdf5', 'r')\n",
    "train_h5 = h.get('data_train')\n",
    "img_x = train_h5.shape[1]\n",
    "img_y = train_h5.shape[2]\n",
    "img_z = train_h5.shape[3]\n",
    "input_shape = (img_x, img_y, img_z)\n",
    "# h.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "pursuant-planning",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319, 319, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputshape = train_h5.shape[1:]\n",
    "inputshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "incredible-packet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(319, 319, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "deluxe-float",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()\n",
    "\n",
    "h = h5py.File('data1.hdf5', 'r')\n",
    "train_h5 = h.get('data_train')\n",
    "img_x = train_h5.shape[1]\n",
    "img_y = train_h5.shape[2]\n",
    "img_z = train_h5.shape[3]\n",
    "input_shape = (img_x, img_y, img_z)\n",
    "h.close()\n",
    "\n",
    "num_classes = len(set(train_label))\n",
    "batch_size = 20\n",
    "epochs = 10\n",
    "\n",
    "x_train = tfio.IODataset.from_hdf5('data1.hdf5', dataset='/data_train')\n",
    "x_test = tfio.IODataset.from_hdf5('data1.hdf5', dataset='/data_test')\n",
    "\n",
    "y_train = tfio.IODataset.from_hdf5('data_label.hdf5', dataset='/data_train')\n",
    "y_test = tfio.IODataset.from_hdf5('data_label.hdf5', dataset='/data_test')\n",
    "\n",
    "train = tf.data.Dataset.zip((x_train,y_train)).batch(batch_size, drop_remainder=True)#.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val = tf.data.Dataset.zip((x_test,y_test)).batch(batch_size, drop_remainder=True)#.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adult-finder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 315, 315, 32)      2432      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 157, 157, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 153, 153, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 76, 76, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 369664)            0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               36966500  \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 37,021,206\n",
      "Trainable params: 37,021,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "authentic-driving",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "303/303 [==============================] - 990s 3s/step - loss: 307.2820 - accuracy: 0.2010 - val_loss: 1.2599 - val_accuracy: 0.6060\n",
      "Epoch 2/10\n",
      "303/303 [==============================] - 993s 3s/step - loss: 1.1002 - accuracy: 0.6173 - val_loss: 0.9460 - val_accuracy: 0.6649\n",
      "Epoch 3/10\n",
      "303/303 [==============================] - 998s 3s/step - loss: 0.5495 - accuracy: 0.8085 - val_loss: 0.7569 - val_accuracy: 0.7262\n",
      "Epoch 4/10\n",
      "303/303 [==============================] - 998s 3s/step - loss: 0.3860 - accuracy: 0.8757 - val_loss: 0.8663 - val_accuracy: 0.6946\n",
      "Epoch 5/10\n",
      "303/303 [==============================] - 979s 3s/step - loss: 0.3413 - accuracy: 0.8918 - val_loss: 0.4892 - val_accuracy: 0.8393\n",
      "Epoch 6/10\n",
      "303/303 [==============================] - 1003s 3s/step - loss: 0.1713 - accuracy: 0.9494 - val_loss: 0.4270 - val_accuracy: 0.8613\n",
      "Epoch 7/10\n",
      "303/303 [==============================] - 992s 3s/step - loss: 0.1058 - accuracy: 0.9711 - val_loss: 0.5619 - val_accuracy: 0.8280\n",
      "Epoch 8/10\n",
      "303/303 [==============================] - 979s 3s/step - loss: 0.0878 - accuracy: 0.9740 - val_loss: 0.7756 - val_accuracy: 0.7863\n",
      "Epoch 9/10\n",
      "303/303 [==============================] - 976s 3s/step - loss: 0.1183 - accuracy: 0.9682 - val_loss: 0.6285 - val_accuracy: 0.8333\n",
      "Epoch 10/10\n",
      "303/303 [==============================] - 978s 3s/step - loss: 0.1044 - accuracy: 0.9656 - val_loss: 0.6948 - val_accuracy: 0.8071\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x199e4c4f8b0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, epochs=epochs, validation_data=val, verbose=1, callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-appearance",
   "metadata": {},
   "source": [
    "## Try to Avert Overfitting 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "placed-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()\n",
    "\n",
    "h = h5py.File('data1.hdf5', 'r')\n",
    "train_h5 = h.get('data_train')\n",
    "img_x = train_h5.shape[1]\n",
    "img_y = train_h5.shape[2]\n",
    "img_z = train_h5.shape[3]\n",
    "input_shape = (img_x, img_y, img_z)\n",
    "h.close()\n",
    "\n",
    "num_classes = len(set(train_label))\n",
    "batch_size = 20\n",
    "epochs = 10\n",
    "\n",
    "x_train = tfio.IODataset.from_hdf5('data1.hdf5', dataset='/data_train')\n",
    "x_test = tfio.IODataset.from_hdf5('data1.hdf5', dataset='/data_test')\n",
    "\n",
    "y_train = tfio.IODataset.from_hdf5('data_label.hdf5', dataset='/data_train')\n",
    "y_test = tfio.IODataset.from_hdf5('data_label.hdf5', dataset='/data_test')\n",
    "\n",
    "train = tf.data.Dataset.zip((x_train,y_train)).batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val = tf.data.Dataset.zip((x_test,y_test)).batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-distribution",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "lonely-spank",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "303/303 [==============================] - 905s 3s/step - loss: 243.8017 - accuracy: 0.1530 - val_loss: 1.4208 - val_accuracy: 0.5274\n",
      "Epoch 2/10\n",
      "303/303 [==============================] - 952s 3s/step - loss: 1.1141 - accuracy: 0.5965 - val_loss: 0.8799 - val_accuracy: 0.7065\n",
      "Epoch 3/10\n",
      "303/303 [==============================] - 964s 3s/step - loss: 0.5767 - accuracy: 0.7994 - val_loss: 0.9851 - val_accuracy: 0.6601\n",
      "Epoch 4/10\n",
      "303/303 [==============================] - 961s 3s/step - loss: 0.4624 - accuracy: 0.8350 - val_loss: 0.7415 - val_accuracy: 0.7506\n",
      "Epoch 5/10\n",
      "303/303 [==============================] - 964s 3s/step - loss: 0.3991 - accuracy: 0.8797 - val_loss: 0.7328 - val_accuracy: 0.7399\n",
      "Epoch 6/10\n",
      "303/303 [==============================] - 965s 3s/step - loss: 0.2810 - accuracy: 0.8968 - val_loss: 0.5679 - val_accuracy: 0.8036\n",
      "Epoch 7/10\n",
      "303/303 [==============================] - 967s 3s/step - loss: 0.1775 - accuracy: 0.9458 - val_loss: 0.6764 - val_accuracy: 0.8018\n",
      "Epoch 8/10\n",
      "303/303 [==============================] - 966s 3s/step - loss: 0.2007 - accuracy: 0.9415 - val_loss: 0.6405 - val_accuracy: 0.8089\n",
      "Epoch 9/10\n",
      "303/303 [==============================] - 966s 3s/step - loss: 0.1252 - accuracy: 0.9665 - val_loss: 0.5559 - val_accuracy: 0.8565\n",
      "Epoch 10/10\n",
      "303/303 [==============================] - 966s 3s/step - loss: 0.2135 - accuracy: 0.9361 - val_loss: 0.5563 - val_accuracy: 0.8363\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x199e7f95f10>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train, epochs=epochs, validation_data=val, verbose=1, callbacks=[history])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-directive",
   "metadata": {},
   "source": [
    "## 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "accurate-bunny",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = History()\n",
    "\n",
    "h = h5py.File('data1.hdf5', 'r')\n",
    "train_h5 = h.get('data_train')\n",
    "img_x = train_h5.shape[1]\n",
    "img_y = train_h5.shape[2]\n",
    "img_z = train_h5.shape[3]\n",
    "input_shape = (img_x, img_y, img_z)\n",
    "h.close()\n",
    "\n",
    "num_classes = len(set(train_label))\n",
    "batch_size = 20\n",
    "epochs = 10\n",
    "\n",
    "x_train = tfio.IODataset.from_hdf5('data1.hdf5', dataset='/data_train')\n",
    "x_test = tfio.IODataset.from_hdf5('data1.hdf5', dataset='/data_test')\n",
    "\n",
    "y_train = tfio.IODataset.from_hdf5('data_label.hdf5', dataset='/data_train')\n",
    "y_test = tfio.IODataset.from_hdf5('data_label.hdf5', dataset='/data_test')\n",
    "\n",
    "train = tf.data.Dataset.zip((x_train,y_train)).batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "val = tf.data.Dataset.zip((x_test,y_test)).batch(batch_size, drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(layers.Conv2D(64, (5, 5), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(100, activation='relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "similar-yellow",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 315, 315, 32)      2432      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 157, 157, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 153, 153, 64)      51264     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 76, 76, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 369664)            0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               36966500  \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 37,021,206\n",
      "Trainable params: 37,021,206\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['AUC'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "sacred-climate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "      8/Unknown - 21s 2s/step - loss: 0.1996 - auc: 0.9970"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-e2c75328691e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#, callbacks=[history])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1098\u001b[0m                 _r=1):\n\u001b[0;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1100\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1101\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"xla\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    853\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    854\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 855\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    856\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    857\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2942\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   2944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1917\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    553\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train, epochs=epochs, validation_data=val, verbose=1)#, callbacks=[history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "domestic-genius",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./tf_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('./tf_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conscious-spouse",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-consolidation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-amazon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seeing-revision",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-stocks",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "heard-strand",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-simulation",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-3.8",
   "language": "python",
   "name": "python-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
